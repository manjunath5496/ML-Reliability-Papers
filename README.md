# ML Reliability Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(1).pdf" style="text-decoration:none;">Intriguing properties of neural networks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(2).pdf" style="text-decoration:none;">Explaining and Harnessing Adversarial Examples</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(3).pdf" style="text-decoration:none;">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(4).pdf" style="text-decoration:none;">DeepFool: a simple and accurate method to fool deep neural networks</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(5).pdf" style="text-decoration:none;">The Limitations of Deep Learning
in Adversarial Settings</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(6).pdf" style="text-decoration:none;">"Why Should I Trust You?" Explaining the Predictions of Any Classifier</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(7).pdf" style="text-decoration:none;">Crafting Adversarial Input Sequences
for Recurrent Neural Networks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(8).pdf" style="text-decoration:none;"> Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(9).pdf" style="text-decoration:none;">Adversarial Training Methods for Semi-Supervised Text Classification</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(10).pdf" style="text-decoration:none;">Adversarial examples in the physical world </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(11).pdf" style="text-decoration:none;">Tactics of Adversarial Attack on Deep Reinforcement Learning Agents</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(12).pdf" style="text-decoration:none;">Feature Squeezing:
Detecting Adversarial Examples in Deep Neural Networks</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(13).pdf" style="text-decoration:none;">DeepXplore: Automated Whitebox Testing
of Deep Learning Systems</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(14).pdf" style="text-decoration:none;">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(15).pdf" style="text-decoration:none;">Towards Deep Learning Models Resistant to Adversarial Attacks</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(16).pdf" style="text-decoration:none;">NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(17).pdf" style="text-decoration:none;">Adversarial Examples for Evaluating Reading Comprehension Systems</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(18).pdf" style="text-decoration:none;">Synthesizing Robust Adversarial Examples</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(19).pdf" style="text-decoration:none;">Robust Physical-World Attacks on Deep Learning Visual Classification</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(20).pdf" style="text-decoration:none;">ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(21).pdf" style="text-decoration:none;">DeepTest: Automated Testing of
Deep-Neural-Network-driven Autonomous Cars</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(22).pdf" style="text-decoration:none;">Generating Natural Adversarial Examples</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(23).pdf" style="text-decoration:none;">Attacking Binarized Neural Networks</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(24).pdf" style="text-decoration:none;">Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(25).pdf" style="text-decoration:none;">Synthetic and Natural Noise Both Break Neural Machine Translation</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(26).pdf" style="text-decoration:none;">Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(27).pdf" style="text-decoration:none;">Adversarial Examples: Attacks and Defenses for Deep Learning</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(28).pdf" style="text-decoration:none;">Query-efficient Black-box Adversarial Examples</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(29).pdf" style="text-decoration:none;">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(30).pdf" style="text-decoration:none;">Certified Defenses against Adversarial Examples</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(31).pdf" style="text-decoration:none;">Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(32).pdf" style="text-decoration:none;">Adversarial Logit Pairing</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(33).pdf" style="text-decoration:none;">DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(34).pdf" style="text-decoration:none;">Generating Natural Language Adversarial Examples</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(35).pdf" style="text-decoration:none;">Greedy Attack and Gumbel Attack:
Generating Adversarial Examples for Discrete Data</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(36).pdf" style="text-decoration:none;">Testing Untestable Neural Machine Translation: An Industrial Case</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(37).pdf" style="text-decoration:none;">Is Robustness the Cost of Accuracy?
&minus; A Comprehensive Study on the Robustness of 18 Deep Image Classication Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(38).pdf" style="text-decoration:none;">Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(39).pdf" style="text-decoration:none;">Adversarial Examples: Opportunities and Challenges</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(40).pdf" style="text-decoration:none;">Adversarial Examples - A Complete Characterisation of the Phenomenon</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(41).pdf" style="text-decoration:none;">Robust Neural Machine Translation with Joint Textual and Phonetic Embedding</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(42).pdf" style="text-decoration:none;">Attacks Meet Interpretability:
Attribute-steered Detection of Adversarial Samples</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(43).pdf" style="text-decoration:none;">On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(44).pdf" style="text-decoration:none;">Improving the Robustness of Speech Translation</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(45).pdf" style="text-decoration:none;">Identifying and Controlling Important Neurons in Neural Machine Translation</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(46).pdf" style="text-decoration:none;">Machine Learning Testing:
Survey, Landscapes and Horizons</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(47).pdf" style="text-decoration:none;">Natural Adversarial Examples</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(48).pdf" style="text-decoration:none;">Hidden Voice Commands</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(49).pdf" style="text-decoration:none;">Towards Evaluating the Robustness
of Neural Networks</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(50).pdf" style="text-decoration:none;">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(51).pdf" style="text-decoration:none;">Fooling Vision and Language Models
Despite Localization and Attention Mechanism</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(52).pdf" style="text-decoration:none;">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(53).pdf" style="text-decoration:none;">On Adversarial Examples for Character-Level Neural Machine Translation</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(54).pdf" style="text-decoration:none;">Defensive Quantization: When Efficiency Meets Robustness </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(55).pdf" style="text-decoration:none;">Differentiable Abstract Interpretation for Provably Robust Neural Networks</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(56).pdf" style="text-decoration:none;">Adversarial Example Generation
with Syntactically Controlled Paraphrase Networks </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(57).pdf" style="text-decoration:none;">NIC: Detecting Adversarial Samples with Neural Network Invariant Checking</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(58).pdf" style="text-decoration:none;">Towards Robust Neural Machine Translation</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(59).pdf" style="text-decoration:none;">Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Reliability-Papers/blob/master/ml(60).pdf" style="text-decoration:none;">Black-Box Attacks against RNN based Malware Detection Algorithms </a></li>
 
   </ul>
     
     
